"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.getLLMConfiguration = void 0;
const azureVariables = [
    'AZURE_OPENAI_API_KEY',
    'AZURE_OPENAI_API_DEPLOYMENT_NAME',
    'AZURE_OPENAI_API_VERSION',
];
const azureBasePath = 'AZURE_OPENAI_BASE_PATH';
const azureInstanceName = 'AZURE_OPENAI_INSTANCE_NAME';
function getLLMConfiguration() {
    var _a;
    let baseUrl = process.env.OPENAI_BASE_URL;
    // Check if the user has configured Azure OpenAI
    // It's configured differently than the other providers, so we need to check for it separately.
    // We're basically coercing the Azure configuration into a single base URL.
    const azureConfigured = azureVariables.every((variable) => process.env[variable]);
    if (azureConfigured) {
        // AZURE_OPENAI_BASE_PATH isn't necessarily <instance_name>.openai.azure.com
        // It can be something like https://westeurope.api.microsoft.com/openai/deployments
        let azureBaseUrl = process.env[azureBasePath];
        if (process.env[azureInstanceName]) {
            azureBaseUrl = `https://${process.env[azureInstanceName]}.openai.azure.com`;
        }
        try {
            if (azureBaseUrl) {
                // This will throw if the URL is invalid.
                new URL(azureBaseUrl);
                baseUrl = azureBaseUrl;
            }
        }
        catch (_b) {
            // Invalid configuration, ignore it. It likely won't work anyway.
        }
    }
    if (!baseUrl && process.env.OPENAI_API_KEY) {
        baseUrl = 'https://api.openai.com';
    }
    return {
        baseUrl,
        model: (_a = process.env.APPMAP_NAVIE_MODEL) !== null && _a !== void 0 ? _a : process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME,
    };
}
exports.getLLMConfiguration = getLLMConfiguration;
//# sourceMappingURL=llmConfiguration.js.map